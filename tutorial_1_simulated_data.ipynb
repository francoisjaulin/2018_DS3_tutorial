{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning for genetic data (Part 1): simulated data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The goal of this practical session is to manipulate high-dimensional, low sample-size data that is typical of many genetic applications. For the sake of illustration, we work first on simulated data, in order to get familiar with the main machine learning techniques. The advantage of simulated data is that we can control the size of the datasets, and the structure of the predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "We first import some packages in Python and fix some graphical parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tell Python we want the figures plotted inline\n",
    "%pylab inline\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "def_colors = prop_cycle.by_key()['color']\n",
    "plt.rc('font', **{'size': 18})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data simulation\n",
    "Let us first simulate a dataset to mimic what could be a set of genotypes, and associated phenotype. The genotype of an individual describes its genome. In a given species, all individuals share more than 99.9% of their DNA, but the relatively few varying positions are responsible for the differences between individuals. Here we focus on single-nucleotide differences, called SNP (Single Nucleotide Polymorphism). In humans, for example, there are a few millions of documented SNP (among 3 billions nucleotide in the full genome), i.e., positions in the genome where we have observed at least two possible nucleotides (among A, C, G, T) in all sequenced individuals. The genotype of an individual is the list of nucleotides at all SNP positions; in humans, this is therefore a list of a few millions entries that characterizes the DNA of each individual.\n",
    "\n",
    "Here, we simulate a species with only 1000 SNPs, to make the following computations easy. We represent a SNP by a binary indicator, which is sufficient when only two nucleotides are possible at each SNP position. We consider a dataset of 150 individuals, to respect the fact that the number of individuals is usually much smaller than the number of SNP in such a study.\n",
    "\n",
    "To each individual we associate a quantitative phenotype (e.g., the size of the individual), and we assume that a few SNPs explain the phenotype following a linear model.\n",
    "\n",
    "In addition, we assume that a network of SNP is given, and that the causal SNP form a module of this network; here we simulate a modular network to mimic this hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features (SNP)\n",
    "num_feats = 1000\n",
    "# Number of samples\n",
    "num_obsvs = 150\n",
    "# Size of modules in the network of features\n",
    "mod_size = 10\n",
    "# Number of causal SNP\n",
    "num_causl = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate features X\n",
    "We create a matrix of SNP in a very naive way. In particular there is no correlation among SNP nor among individuals, which is rarely the case on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.binomial(1, 0.2, size=(num_obsvs, num_feats))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate phenotype y\n",
    "We simulate a phenotype with a sparse linear model. The SNP with nonzero weights in the model form the first module of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random weights\n",
    "w_causl = np.random.normal(loc=0.2, scale=0.05, size=(num_causl))\n",
    "print(w_causl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create phenotypes\n",
    "w = np.zeros((num_feats, ))\n",
    "w[:num_causl] = w_causl\n",
    "\n",
    "y = np.matmul(X, w) + np.random.normal(loc=0., scale=0.1, size=(num_obsvs, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = plt.hist(y, bins=30)\n",
    "plt.xlabel(\"phenotype\", fontsize=18)\n",
    "plt.ylabel(\"frequency\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate network W\n",
    "We create a modular network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.zeros((num_feats, num_feats))\n",
    "for i in range(round(num_feats/mod_size)):\n",
    "    W[i*mod_size:(i+1)*mod_size, i*mod_size:(i+1)*mod_size] = np.ones((mod_size, mod_size))\n",
    "    if not i == (num_feats/mod_size - 1):\n",
    "        W[(i+1)*mod_size-1, (i+1)*mod_size] = 1\n",
    "        W[(i+1)*mod_size, (i+1)*mod_size-1] = 1\n",
    "        \n",
    "# remove the diagonal\n",
    "W = W - np.eye(num_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W[1:12,1:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first 5 modules of the network\n",
    "import networkx as nx\n",
    "G1=nx.from_numpy_matrix(W[1:50,1:50])\n",
    "graph_pos=nx.spring_layout(G1,k=0.50,iterations=50)\n",
    "nx.draw_networkx(G1,graph_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the order of SNP to make it less boring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_indices_l = list(range(num_feats))\n",
    "np.random.shuffle(map_indices_l)\n",
    "map_indices = dict(zip(range(num_feats), map_indices_l))\n",
    "map_indices_r = dict(zip(map_indices_l, range(num_feats)))\n",
    "X = X[:, map_indices_l] \n",
    "W_new = W[map_indices_l, :]\n",
    "W_new = W_new[:, map_indices_l]\n",
    "W = W_new\n",
    "causl = [map_indices_r[ix] for ix in range(num_causl)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split\n",
    "We randomly split the observations into a training set (70% of all data) and a test set (30%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    model_selection.train_test_split(X, y, test_size=0.3, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the size of the different matrices!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the TRUE model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the weights of the true model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize(10, 6))\n",
    "\n",
    "plt.scatter(range(num_feats), # x = SNP position\n",
    "            np.zeros(shape=(num_feats, )), # weight = 0 \n",
    "            s=200) # marker size\n",
    "\n",
    "# Plot the causal SNPs in red\n",
    "plt.scatter(causl, w_causl, s=200)\n",
    "\n",
    "plt.xlabel(\"feature\", fontsize=18)\n",
    "plt.ylabel(\"true feature weight\", fontsize=18)\n",
    "plt.xlim([0, num_feats])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check how good the true model predicts the phenotypes on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: implement the prediction of the model on the test set\n",
    "## y_pred = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "print(\"Percentage of variance explained (using all SNPs): %.2f\\nRMSE: %2f\" % \\\n",
    "    (metrics.explained_variance_score(y_test, y_pred), rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot predicted vs true y\n",
    "fig = plt.figure(figsize(6, 6))\n",
    "\n",
    "plt.scatter(y_test, y_pred, s=200)\n",
    "\n",
    "plt.xlabel(\"y = Xw\", fontsize=18)\n",
    "plt.ylabel(\"y = Xw + noise\", fontsize=18)\n",
    "plt.xlim([np.min(y_test)-0.05, np.max(y_test)+0.05])\n",
    "plt.ylim([np.min(y_test)-0.05, np.max(y_test)+0.05])\n",
    "\n",
    "plt.text(0, 0.6, 'RMSE = %.2f' % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-test\n",
    "\n",
    "Let us start by running a statistical test for association of each SNP feature with the phenotype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalues = []\n",
    "for feat_idx in range(num_feats):\n",
    "    # only look a the column corresponding at that SNP\n",
    "    myX = X_train[:, feat_idx]\n",
    "    # run a linear regression (with bias) between the phenotype and \n",
    "    # this SNP\n",
    "    myX = sm.add_constant(myX)\n",
    "    est = sm.regression.linear_model.OLS(y_train, myX)\n",
    "    est2 = est.fit()\n",
    "    # get the p-value from the model \n",
    "    pvalues.append(est2.pvalues[1])\n",
    "pvalues = np.array(pvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manhattan plot\n",
    "\n",
    "The common way to visualize such results is by using a Manhattan plot: we will plot all SNPs on the x-axis, and on the y-axis we'll have the opposite of the log base 10 of the p-value. The lower the p-value, the higher the corresponding marker. \n",
    "\n",
    "We will also add a horizontal line that corresponds to the _threshold for significance_. Because we are testing multiple hypotheses, we need to lower our threshold accordingly. We will use __Bonferroni correction__ and divide the significance threshold (say, alpha=0.05) by the number of tests, that is, the number of SNPs p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize(10, 6))\n",
    "\n",
    "plt.scatter(range(num_feats), # x = SNP position\n",
    "            -np.log10(pvalues), # y = -log10 p-value \n",
    "            s=200) \n",
    "\n",
    "# Plot the causal SNPs in red\n",
    "plt.scatter(causl, \n",
    "            -np.log10(pvalues[causl]),\n",
    "            color=def_colors[1], s=200)\n",
    "\n",
    "# significance threshold according to Bonferroni correction\n",
    "t = -np.log10(0.05/num_feats)\n",
    "plt.plot([0, num_feats], [t, t], lw=4, c='k')\n",
    "\n",
    "plt.xlabel(\"feature\", fontsize=18)\n",
    "plt.ylabel(\"-log10 p-value\", fontsize=18)\n",
    "plt.xlim([0, num_feats])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a few causal features are identified as significant. Even increasing the significance level (or using a less stringent correction for multiple hypothesis testing) would not rescue the 9 other causal features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "As a first predictive model, let us try the classical ordinary least squares (OLS) method. This is quite bold in a setting where we have more features than observations, but hey, we have to start somewhere!\n",
    "\n",
    "OLS, like many other methods below, is implemented in Scikit Learn and follows a standard syntax to fit a model on training data, and make predictions on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the linear_model module from Scikit Learn\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a variable 'model' as an OLS model, ready to be fit\n",
    "model_OLS = linear_model.LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on training data\n",
    "model_OLS.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have fit your first linear model. Let us look at how it looks, ie, what are the weights of the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize(10, 6))\n",
    "\n",
    "plt.scatter(range(num_feats), # x = SNP position\n",
    "            model_OLS.coef_,  # y = regression weights\n",
    "            s=200)\n",
    "\n",
    "plt.scatter(causl, model_OLS.coef_[causl],\n",
    "            color=def_colors[1], s=200) # plot the weights of the causal SNP in red\n",
    "\n",
    "plt.plot([0, num_feats], [0.025, 0.025], lw=4, c='grey', ls='--') \n",
    "plt.plot([0, num_feats], [-0.025, -0.025], lw=4, c='grey', ls='--')\n",
    "\n",
    "\n",
    "plt.xlabel(\"feature\", fontsize=18)\n",
    "plt.ylabel(\"regression weight\", fontsize=18)\n",
    "plt.xlim([0, num_feats])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features that have the largest weight (in magnitude) are not necessarily the causal ones. Some causal features have a very low weight.\n",
    "\n",
    "The dashed lines at +/- 0.025 have been chosen arbitrarily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive power on the validation set\n",
    "Another way to check if the fitted model is good or not, is to check how well it predits on data that were not used during training. So let's see how well OLS predicts on our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the validation set\n",
    "y_pred_OLS = model_OLS.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess performance\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred_OLS))\n",
    "print(\"Percentage of variance explained (using all SNPs): %.2f\\nRMSE: %2f\" % \\\n",
    "    (metrics.explained_variance_score(y_test, y_pred_OLS), rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot predicted vs true y\n",
    "fig = plt.figure(figsize(6, 6))\n",
    "\n",
    "plt.scatter(y_test, y_pred_OLS, s=200)\n",
    "\n",
    "plt.xlabel(\"true y\", fontsize=18)\n",
    "plt.ylabel(\"prediction\", fontsize=18)\n",
    "plt.xlim([np.min(y_test)-0.05, np.max(y_test)+0.05])\n",
    "plt.ylim([np.min(y_test)-0.05, np.max(y_test)+0.05])\n",
    "\n",
    "plt.text(0, 0.6, 'RMSE = %.2f' % rmse, fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso\n",
    "Lasso regression is a sparse regression techniques, which minimizes the mean squared error penalized by the L1 norm of the predictor. The non-differentiability of the L1 norm leads to sparse solutions, i.e., predictors where only a finite subset of SNP are used in the model. The number of SNPs in the model can be controlled by the regularization parameter which adds more or less penalty.\n",
    "\n",
    "Lasso regression is implemented in Scikit Learn. The regularization parameter is called 'alpha'. Larger alpha's lead to sparser models, i.e., less SNPs selected. Let us for example instanciate a Lasso model with, arbitraritly, a regularization parameter equal to 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l1 = linear_model.Lasso(fit_intercept=True, alpha=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model on the training set with the standard 'fit' method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: fit the model on the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check how the trained model looks like. The coefficients are in the 'coef_' field of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize(12, 8))\n",
    "\n",
    "plt.scatter(range(num_feats), # x = SNP position\n",
    "            model_l1.coef_, # y = regression weight\n",
    "            s=200)\n",
    "\n",
    "plt.scatter(causl, model_l1.coef_[causl],\n",
    "            color=def_colors[1], s=200)\n",
    "\n",
    "plt.xlabel(\"features\", fontsize=18)\n",
    "plt.ylabel(\"lasso weight\", fontsize=18)\n",
    "plt.xlim([0, num_feats])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Many causal SNP are selected, and only a few non-causal SNP are also selected. However, some causal SNP are also not selected. Maybe we regularized too much?\n",
    "\n",
    "A solution to choosed a \"good\" regularization parameter is to test a grid of candidate parameters by cross-validation, and pick the one that gives the best cross-validated performance. Luckily this whole cross-validated optimization is implemented in Scikit Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A grid of parameters\n",
    "alphas = np.logspace(-4., 0., num=20)\n",
    "# A model we want to optimize\n",
    "lasso = linear_model.Lasso(fit_intercept=True)\n",
    "# The cross-validation object\n",
    "model_l1_cv = model_selection.GridSearchCV(lasso, \n",
    "                                        param_grid={'alpha': alphas}, \n",
    "                                        scoring='explained_variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the cross-validated optimization on our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l1_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the best alpha estimated by cross-validation, and how many SNP are selected for the best model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CV performance\n",
    "scores = model_l1_cv.cv_results_['mean_test_score']\n",
    "fig = plt.figure(figsize(10, 6))\n",
    "plt.semilogx(alphas, scores)\n",
    "plt.xlabel(\"alpha\", fontsize=18)\n",
    "plt.ylabel(\"Mean test explained variance\", fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best alpha: %.4f\" % \\\n",
    "      model_l1.best_estimator_.alpha)\n",
    "print(\"%d SNPs selected.\" % \\\n",
    "    np.nonzero(model_l1.best_estimator_.coef_)[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"best\" model, with the alpha parameters that optimizes the cross-validated performance, is stored in the 'best_estimator_' field. Let's look at its weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize(12, 8))\n",
    "\n",
    "plt.scatter(range(num_feats), # x = SNP position\n",
    "            model_l1.best_estimator_.coef_, # y = regression weight\n",
    "            s=200)\n",
    "\n",
    "plt.scatter(causl, model_l1.best_estimator_.coef_[causl],\n",
    "            color=def_colors[1], s=200)\n",
    "\n",
    "plt.xlabel(\"features\", fontsize=18)\n",
    "plt.ylabel(\"lasso weight\", fontsize=18)\n",
    "plt.xlim([0, num_feats])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is sparse, but many non-causal features have non-zero weights. It is a general property of the LASSO that when performance is optimized, it is better to include too many features than not enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now look at how good the model predicts the phenotype on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Predict on the validation set\n",
    "## y_pred = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess performance\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "print(\"Percentage of variance explained (using all SNPs): %.2f\\nRMSE: %2f\" % \\\n",
    "    (metrics.explained_variance_score(y_test, y_pred), rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot predicted vs true y\n",
    "fig = plt.figure(figsize(6, 6))\n",
    "\n",
    "plt.scatter(y_test, y_pred, s=200)\n",
    "\n",
    "plt.xlabel(\"true y\", fontsize=18)\n",
    "plt.ylabel(\"prediction\", fontsize=18)\n",
    "plt.xlim([np.min(y_test)-0.05, np.max(y_test)+0.05])\n",
    "plt.ylim([np.min(y_test)-0.05, np.max(y_test)+0.05])\n",
    "\n",
    "plt.text(0, 0.6, 'RMSE = %.2f' % rmse, fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic net\n",
    "Elastic net is a generalization of Lasso. It also creates sparse models, but can be better than lasso in particular in the presence of correlated features.\n",
    "\n",
    "Elastic net regularizes the mean squared error by a sum of two penalties: a L1 penalty (like the lasso), and a L2 penalty. It therefore has two regularization parameters, which are encoded in Scikit Learn as 'alpha' (the total amount of regularization), and 'l1_ratio' (the fraction of L1 penalty compared to L2). Setting 'l1_ratio' to 1 boils down to standard Lasso regression.\n",
    "\n",
    "As for Lasso, we can automatically choose the parameters by cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters grid\n",
    "alphas = np.logspace(-3., 1., num=15)\n",
    "ratios = np.linspace(0.5, 1., num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the elastic net model\n",
    "enet = linear_model.ElasticNet(fit_intercept=True)\n",
    "# Define the cross-validation procedure\n",
    "model_l1l2 = model_selection.GridSearchCV(enet, \n",
    "                                        param_grid={'alpha': alphas, \n",
    "                                                    'l1_ratio': ratios}, \n",
    "                                        scoring='explained_variance')\n",
    "# Train the elastic net model with parameter optimization by cross-validation\n",
    "## TODO: train on the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let's look at the weights of the models, and how well it predicts phenotypes on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize(12, 8))\n",
    "\n",
    "plt.scatter(range(num_feats), # x = SNP position\n",
    "            model_l1l2.best_estimator_.coef_, # y = regression weight\n",
    "            s=200)\n",
    "\n",
    "plt.scatter(causl, model_l1l2.best_estimator_.coef_[causl],\n",
    "            color=def_colors[1], s=200)\n",
    "\n",
    "plt.xlabel(\"features\", fontsize=18)\n",
    "plt.ylabel(\"elastic net weight\", fontsize=18)\n",
    "plt.xlim([0, num_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the validation set\n",
    "## TODO: predict on the test set\n",
    "\n",
    "# Assess performance\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "print(\"Percentage of variance explained (using all SNPs): %.2f\\nRMSE: %2f\" % \\\n",
    "    (metrics.explained_variance_score(y_test, y_pred), rmse))\n",
    "\n",
    "# Scatter plot predicted vs true y\n",
    "fig = plt.figure(figsize(6, 6))\n",
    "\n",
    "plt.scatter(y_test, y_pred, s=200)\n",
    "\n",
    "plt.xlabel(\"true y\", fontsize=18)\n",
    "plt.ylabel(\"prediction\", fontsize=18)\n",
    "plt.xlim([np.min(y_test)-0.05, np.max(y_test)+0.05])\n",
    "plt.ylim([np.min(y_test)-0.05, np.max(y_test)+0.05])\n",
    "\n",
    "plt.text(0, 0.6, 'RMSE = %.2f' % rmse, fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability selection\n",
    "\n",
    "Stability selection is a technique to improve the feature selection process of a given procedure, here the Lasso. Instead of running the Lasso only once to select features, stability selection runs the Lasso many times, on randomly perturbed training sets (by subsampling examples and reweighting variables). The features are then scored by how frequently they are selected by the lasso, and the most highly scored features are kept.\n",
    "\n",
    "Although deprecated, Scikit Learn as a convenient implementation of stability selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A stability selection object (we just need to fix a value for alpha)\n",
    "randlasso = linear_model.RandomizedLasso(\n",
    "    alpha=model_l1_cv.best_params_['alpha'])\n",
    "# Train the model\n",
    "## TODO: train on the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the scoring of the features by stability selection. Note that the vertical axis is not the weights of a model, but the selection frequency of each feature during stability selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize(12, 8))\n",
    "\n",
    "plt.scatter(range(num_feats), # x = SNP position\n",
    "            randlasso.scores_, # y = regression weight\n",
    "            s=200)\n",
    "\n",
    "plt.scatter(causl, randlasso.scores_[causl],\n",
    "            color=def_colors[1], s=200)\n",
    "\n",
    "plt.xlabel(\"features\", fontsize=18)\n",
    "plt.ylabel(\"stability selection frequency\", fontsize=18)\n",
    "plt.xlim([0, num_feats])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network-constrained Lasso\n",
    "\n",
    "Network-constrained Lasso (ncLasso) is a variant of elastic net, and requires the knowledge of a graph to describe relationships between features. The main assumptions behind ncLasso are not only that the model is sparse (like Lasso or elastic net), but also that the features with non-zero weights of the model tend to be connected in the network, and to have similar weights when they are connected.\n",
    "\n",
    "Mathematically, ncLasso follows elastic net by minimizing a sum of squared error criterion penalized by two regularization, a L1 penalty (to enforce sparsity), and a graph-modified L2 penalty. The graph-modified L2 penalty enforces smoothness of the coefficients on the graph.\n",
    "\n",
    "This method, developed by [Li and Li (2008)](https://academic.oup.com/bioinformatics/article/24/9/1175/206444) is not implemented in scikit-learn, so we'll need to create our own estimator.\n",
    "\n",
    "It turns out that it is possible to transform the network-constrained Lasso problem into a Lasso problem: follow [the original paper](https://academic.oup.com/bioinformatics/article/24/9/1175/206444) (pdf also available [here](http://www.stat.purdue.edu/~doerge/BIOINFORM.D/FALL10/Li_and_Li_2008_Bioinformatics.pdf) and the note in section C of [the supplementary material of Sugiyama et al. (2014)](http://cazencott.info/dotclear/public/publications/sugiyama2014_supp.pdf) to replace the eigen-decomposition of the graph Laplacian with the graph incidence matrix.\n",
    "\n",
    "Follow the [documentation](http://scikit-learn.org/stable/developers/contributing.html#rolling-your-own-estimator) or this [blog post](http://danielhnyk.cz/creating-your-own-estimator-scikit-learn/) to create a scikit-learn estimator.\n",
    "\n",
    "Be careful: the computations might require a lot of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import base, linear_model\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ncLasso(base.BaseEstimator, base.RegressorMixin):\n",
    "    def __init__(self, LapSqrt=None, lambda1=1.0, lambda2=1.0):\n",
    "        self.LapSqrt = LapSqrt # sparse matrix\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        \n",
    "    def fit(self, X, y):       \n",
    "        self.lasso = linear_model.Lasso(fit_intercept=True, alpha=self.lambda1)\n",
    "        \n",
    "        ## TODO:\n",
    "        ## y_new = ...\n",
    "        ## X_new = ...\n",
    "        \n",
    "        self.lasso.fit(X_new, y_new)\n",
    "        ## TODO:\n",
    "        ## self.coef_ = ...\n",
    "        \n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def predict(self, X, y=None):\n",
    "        ## TODO:\n",
    "        ## return ...                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compute a square root of the Laplacian by eigenvector decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = np.sum(W, axis=0)\n",
    "L = np.diag(degree) - W\n",
    "\n",
    "# spectral decomposition\n",
    "evals, evecs = np.linalg.eigh(L)\n",
    "\n",
    "# correct for numerical errors: \n",
    "# eigenvalues of 0 might be computed as small negative numbers\n",
    "evals = np.maximum(0, evals)\n",
    "\n",
    "# Square root\n",
    "S = np.dot(np.diag(np.sqrt(evals)), evecs.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we have a correct square root\n",
    "print(np.max(np.abs(np.matmul(S.T, S) - L)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit a ncLasso with lambda#=0.002 and lambda2=2.\n",
    "## model_ncLasso = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize(12, 8))\n",
    "\n",
    "plt.scatter(range(num_feats), # x = SNP position\n",
    "            model_ncLasso.coef_, # y = regression weight\n",
    "            s=200)\n",
    "\n",
    "plt.scatter(causl, model_ncLasso.coef_[causl],\n",
    "            color=def_colors[1], s=200)\n",
    "\n",
    "plt.xlabel(\"features\", fontsize=18)\n",
    "plt.ylabel(\"ncLasso weight\", fontsize=18)\n",
    "plt.xlim([0, num_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the validation set\n",
    "y_pred = model_ncLasso.predict(X_test)\n",
    "\n",
    "# Assess performance\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "print(\"Percentage of variance explained (using all SNPs): %.2f\\nRMSE: %2f\" % \\\n",
    "    (metrics.explained_variance_score(y_test, y_pred), rmse))\n",
    "\n",
    "fig = plt.figure(figsize(6, 6))\n",
    "\n",
    "plt.scatter(y_test, y_pred, s=200)\n",
    "\n",
    "plt.xlabel(\"true y\", fontsize=18)\n",
    "plt.ylabel(\"prediction\", fontsize=18)\n",
    "plt.xlim([np.min(y_test)-0.05, np.max(y_test)+0.05])\n",
    "plt.ylim([np.min(y_pred)-0.05, np.max(y_pred)+0.05])\n",
    "\n",
    "plt.text(0, 0.6, 'RMSE = %.2f' % rmse, fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another square root of the Laplacian is the incidence matrix. Can you run the same experiment with the incidence matrix instead of computing the eigendecomposition of the Laplacian?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
